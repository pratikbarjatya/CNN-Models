{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"GAN.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"RA6Qe6oaeD-V","colab_type":"text"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"360\" height=\"160\" /></center>"]},{"cell_type":"markdown","metadata":{"id":"SdM1B2wmaVYA","colab_type":"text"},"source":["# Generative Adversarial Network"]},{"cell_type":"markdown","metadata":{"id":"kUjzR7WPaVYB","colab_type":"text"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"1I3z-YOXaVYC","colab_type":"text"},"source":["In **supervised learning**, we have data **'X'** and respone(label) **'Y'** and the goal is to learn a function to map x to y e.g. **regression**, **classification**, **object detection**;\n","\n","In **unsupervised learning**, there are no labels and the goal is to find some underlying hidden structures of the data e.g. **clustering**, **dimesnionality reduction**, **feature learning**.\n","\n","<br> \n","The **goal** of generative models is to **generate new samples** of data from a distribution.\n","\n","- These models are used in problems such as **density estimation**, a problem of unsupervised learning."]},{"cell_type":"markdown","metadata":{"id":"KbpAogRoaVYD","colab_type":"text"},"source":["Generative Adversarial Networks (GANs) is a powerful class of neural networks that are used for **unsupervised learning**.\n","\n","- It was developed and introduced by Ian J.Goodfellow in 2014.\n","\n","- Basically made up of a system of **two competing neural network models** which compete with each other and are able to **analyze**, **capture** and **copy** the variations within a dataset."]},{"cell_type":"markdown","metadata":{"id":"RpVQG5UVaVYI","colab_type":"text"},"source":["## Generative model vs Discriminative model"]},{"cell_type":"markdown","metadata":{"id":"Q4cN04ZxaVYJ","colab_type":"text"},"source":["- The discriminative models learn the *conditional* probability distribution P(y|x) i.e. the decision boundary between classes e.g. logistic regression, neural network.\n","\n","- The Generatice models learn the *joint* probability distribution P(x|y) i.e. the distribution of individual classes e.g. Naive Bayes, Gaussian discriminant analysis(GDA).\n","\n","<br> \n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/g.png\"></center>"]},{"cell_type":"markdown","metadata":{"id":"5_ZQtnM2aVYK","colab_type":"text"},"source":["### Why were GANs developed in the first place?"]},{"cell_type":"markdown","metadata":{"id":"qnWAAJrmaVYK","colab_type":"text"},"source":["- Most of the mainstream neural nets can be easiy fooled into **misclassifying things** by adding only a **small amount of noise** into the original data.\n","\n","- Surprisingly, the model **after** adding noise has **higher confidence** in the wrong **prediction** than when it predicted correctly.\n","\n","- The reason for such adversary is the most machine learning models learn from a limited amount of data, which is a huge drawback, as it is **prone to overfitting**.\n","\n","- Mapping between the input and output is almost **linear**.\n","\n","- Although, it may seem that the **boundaries of seperation** between the various classes are linear, but in reality, they are composed of linearities and even a **small change** in a point in the feature space might lead to **misclassification** of data."]},{"cell_type":"markdown","metadata":{"id":"Pk72WlydaVYL","colab_type":"text"},"source":["### Let's understand using an example"]},{"cell_type":"markdown","metadata":{"id":"bySyaBWPaVYM","colab_type":"text"},"source":["Consider a example of the process of money counterfeiting.\n","\n","In this process, we can imagine, two types of agents:\n","\n","- **A Criminal**\n","\n","- **A Cop**"]},{"cell_type":"markdown","metadata":{"id":"eTAs_FA8aVYN","colab_type":"text"},"source":["Let's look their competing objectives.\n","\n","- **Criminal's Objective:** The main objective of the criminal is to come up with complex ways of counterfeiting money such that the **Cop** cannot distinguish between counterfeited money and real money.\n","\n","- **Cop's Objective:** The main objective of the top is to come up with complex ways so as to distinguish between counterfeited money and real money."]},{"cell_type":"markdown","metadata":{"id":"7gaccrsqaVYO","colab_type":"text"},"source":["As this progresses the **cop** develops more and more sphisticated technology to detect money counterfeiting and **criminal** develops more and more sophisticated technology to counterfeit money.\n","\n","This is the basis of what is called an **Adversarial Process**."]},{"cell_type":"markdown","metadata":{"id":"8vrpHbv6aVYP","colab_type":"text"},"source":["### How does GANs work?"]},{"cell_type":"markdown","metadata":{"id":"_Q4GZYFsaVYQ","colab_type":"text"},"source":["The underlying idea is to use two neural networks instead of one.\n","\n","The training and learning process stay the same and utilize the standard techniques (like backpropagation).\n","\n","However, this time we train not one, but two models:\n","\n","- A Generative Model (G) and,\n","\n","- A Discriminative Model (D)\n","\n","<br> \n","Generative Adversarial Networks can be broken down into **three parts**:\n","\n","- **Generative**: To learn a generative model, which describes how data is generated in terms of a **probabilistic model**.\n","\n","- **Adversarial**: The **training** of a model is done in an **adversarial setting**.\n","\n","- **Networks**: Use deep neural networks as the Artificial Intelligence(AI) algorithms for **training purpose**."]},{"cell_type":"markdown","metadata":{"id":"i7RvdHueaVYQ","colab_type":"text"},"source":["In GANs, there is a **GENERATOR** and a **DISCRIMINATOR**.\n","\n","- The Generator generates **fake sample** of data (be it an image, audio, etc.) and **tries to fool** the Discriminator.\n","\n","- The Discriminator, on the other hand, tries to **distinguish** between the **real and fake samples**.\n","\n","- The Generator and Discriminator are both Neural Networks and they both run in **competition** with each other in the **training phase**.\n","\n","- The steps are repeated several times and in this, the Generator and Discriminator get **better and better** in their respective jobs after **each repetition**.\n","\n","<br> \n","- **Check the below flowchart:**"]},{"cell_type":"markdown","metadata":{"id":"8fPzh4InaVYS","colab_type":"text"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/gan1.jpg\"></center>"]},{"cell_type":"markdown","metadata":{"id":"insAk3gyaVYS","colab_type":"text"},"source":["### Understanding it's working"]},{"cell_type":"markdown","metadata":{"id":"lE7kbag1aVYT","colab_type":"text"},"source":["Here, \n","\n","- The generative model captures the distribution of data and is trained in such a manner that it tries to maximize the probability of the Discriminator in making a mistake.\n","\n","- The Discriminator, on the other hand, is based on a model that estimates the probability that the sample that it got is received from the training data and not from the Generator.\n","\n","- The GANs are formulated as a minimanx game, where the Discriminator is trying to minimize its reward **V(D, G)** and the generator is trying to minimize the Discriminator's reward to minimize the Discriminator's reward or inother words, maximize its loss."]},{"cell_type":"markdown","metadata":{"id":"u6cgBoT9aVYU","colab_type":"text"},"source":["**Mathematically**,"]},{"cell_type":"markdown","metadata":{"id":"djvkqcRmaVYV","colab_type":"text"},"source":["<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/gan2.png\"><center>"]},{"cell_type":"markdown","metadata":{"id":"bQT-k3bcaVYW","colab_type":"text"},"source":["**where**,\n","\n","- G = Generator\n","\n","- D = Discriminator\n","\n","- Pdata(x) = distribution of real data\n","\n","- P(z) = distribution of generator\n","\n","- x = sample from Pdata(x)\n","\n","- z = sample from P(z)\n","\n","- D(x) = Discriminator network\n","\n","- G(z) = Generator network"]},{"cell_type":"markdown","metadata":{"id":"MGWqwbkPaVYW","colab_type":"text"},"source":["### Training a GAN"]},{"cell_type":"markdown","metadata":{"id":"Vjc4eV8QaVYX","colab_type":"text"},"source":["**GAN Network**\n","\n","<br> \n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/g1.png\"></center>"]},{"cell_type":"markdown","metadata":{"id":"4nTbiuAdaVYY","colab_type":"text"},"source":["<br> \n","Training a GAN has **two parts**.\n","\n","Training involves alternating between training the discriminator and the generator.\n","\n","The **real loss** and **fake loss** are used to calculate the discriminator losses as follows:\n","\n","<br> \n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/g2.png\"></center>"]},{"cell_type":"markdown","metadata":{"id":"ykBG4kgoaVYZ","colab_type":"text"},"source":["<br> \n","**Part 1**:\n","\n","- The Discriminator is trained while the Generator is idle.\n","\n","- In this phase, the network is only forward propagated and no back-propagaton is done.\n","\n","- The Discriminator is trained on real data for n epochs and see if it can correctly predict them as real.\n","\n","- Also, in this phase, the Discriminator is also trained on the fake generated data from the Generator and see it can correctly predict them as fake."]},{"cell_type":"markdown","metadata":{"id":"iBjJf-IFaVYa","colab_type":"text"},"source":["**Part 2**:\n","\n","- The Generator is trained while the Discriminator is idle.\n","\n","- After the Disciminator is trained by the generated fake data of the Generator, we can get its predictions and use the results for training the Generator and get better from the previous state to try and fool the DIscriminator."]},{"cell_type":"markdown","metadata":{"id":"1o-BK96TaVYb","colab_type":"text"},"source":["The above method is repeated for a few epochs and then manually check the fake data if it seems genuine.\n","\n","If it seems acceptable, then the training is stopped, otherwise, its allowed to continue for few more epochs."]},{"cell_type":"markdown","metadata":{"id":"65gS9lseaVYc","colab_type":"text"},"source":["### Discriminator (D) training"]},{"cell_type":"markdown","metadata":{"id":"flpbTQMgaVYc","colab_type":"text"},"source":["For the real images, we want D(real images) = 1.\n","\n","- This means we want the discriminator to classify then the real images with a label = 1, indicating that these are real.\n","\n","- The discriminator loss for the fake data is similar.\n","\n","- We want D(fake images) = 0, where the fake images are the generator output, fake_images = G(z).\n","\n","  1. Compute the discriminator loss on real, training images.\n","\n","  2. Generate fake images\n","\n","  3. Compute the discriminator loss on fake, generated images\n","\n","  4. Add up real and fake loss to get total loss\n","\n","  5. Perform backpropagation + an optimization step to update the discriminator's weights."]},{"cell_type":"markdown","metadata":{"id":"_e8EvmZQaVYd","colab_type":"text"},"source":["### Generator (G) training"]},{"cell_type":"markdown","metadata":{"id":"trholyMZaVYe","colab_type":"text"},"source":["The generator's goal is to get D(fake images) = 1.\n","\n","  1. Generate fake images.\n","\n","  2. Compute the discriminator loss on fake images, using flipped labels.\n","\n","  3. Perform backpropagation + an optimization step to update the generator's weights."]},{"cell_type":"markdown","metadata":{"id":"SLo3nZPlaVYf","colab_type":"text"},"source":["### Different types of GAN's"]},{"cell_type":"markdown","metadata":{"id":"8fuBLXsFaVYf","colab_type":"text"},"source":["GANs are now a very active topics of research and there have been many different types of GAN implementation.\n","\n","Some of the important ones that are actively being used currently are described below:"]},{"cell_type":"markdown","metadata":{"id":"ZkJegOxraVYg","colab_type":"text"},"source":["**Vanilla GAN**\n","\n","- This is the simplest type GAN.\n","\n","- The Generator and the Discriminator are simple multi-layer perceptrons.\n","\n","- In vanilla GAN, the algorithm is really simple, it tries to potimize the mathematical equation using stochastics gradient descent."]},{"cell_type":"markdown","metadata":{"id":"9ZUC58C_aVYh","colab_type":"text"},"source":["**Conditional GAN(CGAN)**\n","\n","- CGAN can be described as a deep learning method in which some conditional parameters are put into place.\n","\n","- An additional parameter 'y' is added to the Generator for generating the corresponding data.\n","\n","- Labels are also put into the input to the Discriminator in order for the Discriminator to help distinguish the real data from the fake generated data. "]},{"cell_type":"markdown","metadata":{"id":"bsilV1mjaVYi","colab_type":"text"},"source":["**Deep Convolutional GAN(DCGAN)**\n","\n","- DCGAN is one of the most popular also the most successful implementation of GAN.\n","\n","- It is composed of ConvNets in place of multi-layer perceptrons.\n","\n","- The ConvNets are implemented without max pooling, which is in fact replaced by convolutional stride.\n","\n","- The layers are not fully connected."]},{"cell_type":"markdown","metadata":{"id":"pMM4u6vVaVYj","colab_type":"text"},"source":["**Laplacian Pyramid GAN (LAPGAN)**\n","\n","- Laplacian pyramid is linear ivertible image representation consisting of a set of band-pass images, spaced an octave apart, plus a low-frequency residual.\n","\n","- This uses multiple numbers of Generator and Discriminator networks and different levels of the Laplacian Pyramid.\n","\n","- Used mainly because it produces very high-quality images.\n","\n","- The image is downsampled at first at each layer of the pyramid and then it is again up-scaled at each layer in a backward pass where the image acquires some noise from the Conditional GAN at these layers until it reaches its original size."]},{"cell_type":"markdown","metadata":{"id":"aI4200TcaVYk","colab_type":"text"},"source":["**Super Resolution GAN(SRGAN)**"]},{"cell_type":"markdown","metadata":{"id":"G7_X37x0aVYk","colab_type":"text"},"source":["- As the name suggests is a way of designing GAN in which a deep neural network is used along with an adversarial network in order to produce higher resolution images.\n","\n","- This type of GAN is particularly useful in optimally up-scaling native low-resolution images to enhance its details minimizing errors while doing so."]}]}